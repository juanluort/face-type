<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>FACE/TYPE</title>
  <style>
    body {
      margin: 0;
      background: #111;
      color: white;
      font-family: 'Recursive', sans-serif;
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100vh;
      overflow: hidden;
    }

    #text {
      font-size: 3rem;
      transition: all 0.5s ease;
      text-align: center;
      padding: 1rem;
    }

    .happy {
      font-variation-settings: "wght" 800, "slnt" -10;
      color: #ffd166;
    }

    .sad {
      font-variation-settings: "wght" 300, "slnt" 10;
      color: #118ab2;
    }

    .angry {
      font-variation-settings: "wght" 900, "slnt" 0;
      color: #ef476f;
    }

    .neutral {
      font-variation-settings: "wght" 400, "slnt" 0;
      color: #ffffff;
    }

    video {
      position: fixed;
      top: 1rem;
      left: 1rem;
      width: 200px;
      border: 2px solid white;
      z-index: 10;
    }
  </style>
  <link href="https://fonts.googleapis.com/css2?family=Recursive:slnt,wght@-10..0,300..900&display=swap" rel="stylesheet">
</head>
<body>
  <div id="text" class="neutral">Tu rostro cambia la tipografía.</div>
  <video id="video" width="300" height="225" autoplay muted></video>

  <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
  <script>
    window.addEventListener('load', () => {
      const video = document.getElementById('video');
      const text = document.getElementById('text');

      function startVideo() {
        console.log("Modelos cargados. Solicitando cámara…");
        navigator.mediaDevices.getUserMedia({ video: true })
          .then(stream => {
            video.srcObject = stream;
            console.log("Cámara activa.");
          })
          .catch(err => console.error("Error con la cámara:", err));
      }

      Promise.all([
        faceapi.nets.tinyFaceDetector.loadFromUri('models/tiny_face_detector'),
        faceapi.nets.faceExpressionNet.loadFromUri('models/face_expression')
      ]).then(startVideo);

      video.addEventListener('play', () => {
        const canvas = faceapi.createCanvasFromMedia(video);
        document.body.append(canvas);
        const displaySize = { width: video.width, height: video.height };
        faceapi.matchDimensions(canvas, displaySize);

        setInterval(async () => {
          const detection = await faceapi
            .detectSingleFace(video, new faceapi.TinyFaceDetectorOptions())
            .withFaceExpressions();

          console.log("Detección:", detection);

          if (detection && detection.expressions) {
            const expr = detection.expressions;
            const dominant = Object.entries(expr).reduce((a, b) => a[1] > b[1] ? a : b)[0];
            console.log("Emoción detectada:", dominant);
            text.className = ['happy', 'sad', 'angry'].includes(dominant) ? dominant : 'neutral';
          } else {
            text.className = 'neutral';
          }
        }, 1000);
      });
    });
  </script>
</body>
</html>
